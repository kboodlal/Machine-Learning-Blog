[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning Blog",
    "section": "",
    "text": "Clustering\n\n\n\n\n\n\n\nClustering\n\n\nMachine Learning\n\n\n\n\n\n\n\n\n\n\n\nDec 4, 2023\n\n\nKeaton Boodlal\n\n\n\n\n\n\n  \n\n\n\n\nClassification Blog\n\n\n\n\n\n\n\nClassification\n\n\nMachine Learning\n\n\n\n\n\n\n\n\n\n\n\nDec 3, 2023\n\n\nKeaton Boodlal\n\n\n\n\n\n\n  \n\n\n\n\nOutlier Detection Blog\n\n\n\n\n\n\n\nOutlier Detection\n\n\nMachine Learning\n\n\n\n\n\n\n\n\n\n\n\nDec 2, 2023\n\n\nKeaton Boodlal\n\n\n\n\n\n\n  \n\n\n\n\nLinear Regression Blog\n\n\n\n\n\n\n\nLinear Regression\n\n\nMachine Learning\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2023\n\n\nKeaton Boodlal\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Classification/index.html",
    "href": "posts/Classification/index.html",
    "title": "Classification Blog",
    "section": "",
    "text": "For my classification project, I decided to do a fake news detector using python. In recent times, the spread of fake news has been increasing due to the internet. This issue was at an all-time high during the 2016 election. Even though fake news has been around for a long time, through propaganda and other methods, it isn’t as effective as it is today due to the internet.\nEssentially, what it does is processes the words from the article, and then decides if it is fake or not using data from previously classified articles.\nUsing a dataset containing 6335 classified articles, I removed the unnecessary columns. Then I displayed a bar graph with the type of article as the classifier.  I also randomized the order of the data so the model isn’t biased.\nnews = pandas.read_csv('News.csv', index_col=0)\nnews = news.sample(frac=1)\nnews.reset_index(inplace=True)\nnews.drop(['title'], axis=1)\nprint(news.shape)\nsns.countplot(data=news,\n                  x='label',\n                  order=news['label'].value_counts().index)\nplt.xlabel(\"Type of News\")\nplt.title(\"Distribution of Data\")\nplt.show()\n\nThen I split the data into 4 different sets and proceeded to train the Machine learning model using the TfidfVectorizer and a Passive Aggressive classifier. After training the model I tested it and it consistently gets around 93.5% accuracy on determining whether an article is fake news. After completing the training and testing the model, I think made a confusion matrix using the data.\nx_train, x_test, y_train, y_test = train_test_split(news['text'], news['label'],\n                                                    test_size=0.25, random_state=10)\nvectorizer = TfidfVectorizer(stop_words='english', max_df=.9)\ntrain = vectorizer.fit_transform(x_train)\ntest = vectorizer.transform(x_test)\nclassifier = PassiveAggressiveClassifier(max_iter=100)\nclassifier.fit(train, y_train)\npred = classifier.predict(test)\nprint('Accuracy', accuracy_score(y_test, pred) * 100)\ncm = confusion_matrix(y_test, pred, labels=['FAKE', 'REAL'])\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm,\n                              display_labels=['FAKE', 'REAL'])\ndisp.plot()\nplt.show()\n\nIn conclusion, I think this project was successful overall, being able to predict fake news with around a 93% accuracy is what I consider good. Overall I am satisfied with how it performs and the speed at which it processes all the data.\nGithub View Source button takes you to the source code for the .qmd file.\nClassification Jupiter Notebook"
  },
  {
    "objectID": "posts/Clustering/index.html",
    "href": "posts/Clustering/index.html",
    "title": "Clustering",
    "section": "",
    "text": "For this project on clustering, I took weight, height and gender data, and did a K-mean clustering algorithm to divide it into 2 clusters to see how closely these clusters divided through height and weight would relate to gender.\nFirst I started by importing the needed libraries and importing the data.\nimport matplotlib\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas\nimport random as rd\nimport seaborn as sns\nimport sys\nimport os\npath = '/content/drive/My Drive/Colab Notebooks/'\nW_H_G_data = os.path.join(path, 'weight-height.csv')\nfrom google.colab import drive\ndrive.mount('/content/drive')\nAfter importing the data set and libraries, I plotted the data to see the distribution based on gender.\ndata = pandas.read_csv(W_H_G_data)\nsns.displot(data=data['Gender']).set(title=\"Distribution\", xlabel=\"Gender\",\n                                                ylabel=\"Number of People\")\n\nAs displayed by the graph, the data is even between men and women, with 5000 data points each.\nSince we know the split of the data, I decided to plot men and women separately on the same plot to see the difference between the two. With Male being graphed in red and Female being graphed in blue.\ndata_male = data[data['Gender'] == 'Male']\ndata_female = data[data['Gender'] == 'Female']\nplt.scatter(data_male['Height'], data_male['Weight'], color='red')\nplt.scatter(data_female['Height'], data_female['Weight'], color='blue')\nplt.xlabel('Height')\nplt.ylabel('Weight')\nplt.title('Weight and Height by Gender')\n\nI then made a k-means algorithm using 2 as the number of clusters, one cluster for men and one for women. After separating the genders from the data, I used the algorithm to separate the data around 2 centroids, changing the centroid with each iteration, iterating until the centroid doesn’t change. Then I graphed the results with the final centroids in black.\ndata_set = data[['Height','Weight']]\nK =2\n\n# Select random observation as centroids\ncent = (data_set.sample(n=K))\ndiff = 1\nj=0\n\nwhile(diff!=0):\n    inter_data=data_set\n    i=1\n    for index1,row_c in cent.iterrows():\n        diff_list=[]\n        for index2,row_d in inter_data.iterrows():\n            d=np.sqrt((row_c['Weight']-row_d['Weight'])**2 + (row_c['Height']-row_d['Height'])**2)\n            diff_list.append(d)\n        data_set[i] = diff_list\n        i += 1\n\n    C=[]\n    for index,row in data_set.iterrows():\n        min_dist=row[1]\n        pos=1\n        for l in range(K):\n            if row[l+1] &lt; min_dist:\n                min_dist = row[l+1]\n                pos=l+1\n        C.append(pos)\n    data_set['Cluster']=C\n    new_cent = data_set.groupby(['Cluster']).mean()[['Weight','Height']]\n    if j == 0:\n        diff=1\n        j += 1\n    else:\n        diff = (new_cent['Weight'] - cent['Weight']).sum() + (new_cent['Height'] - cent['Height']).sum()\n    cent = data_set.groupby(['Cluster']).mean()[['Weight','Height']]\n    \ncolor=['blue','red']\nfor k in range(K):\n    data=data_set[data_set['Cluster']==k+1]\n    plt.scatter(data['Height'],data['Weight'],c=color[k])\nplt.scatter(cent['Height'],cent['Weight'],c='black')\nplt.xlabel('Height')\nplt.ylabel('Weight')\nplt.show()\n\nAs can seen in this scatter plot, the k-mean prediction of men and women, is relatively consistent with the initial graph showing the distribution of data.\nGithub View Source button takes you to the source code for the .qmd file.\nClustering Notebook"
  },
  {
    "objectID": "posts/Linear_regression/index.html",
    "href": "posts/Linear_regression/index.html",
    "title": "Linear Regression Blog",
    "section": "",
    "text": "For my project on linear regression, I used data from a game called League of Legends. League of Legends is a team-based game involving 2 teams, 5 players on each team. In this game, players use gold to buy items which increase their strength for the team fights. There are 5 different roles, with over 100 different legends. For this project, I decided to see how a character’s creep score relates to their gold. Since you get gold for each creep score you have, it would make sense that the relationship is linear, however, getting creep score is not the only way for one to get gold in the game, it is just a method to do so.\nI used python as the programming language for this project. Using data from millions of games, I took the average of most characters creep score, filtered out support, a role that generally doesn’t really on creep score for gold, and then plotted it. I then added a linear regression model. Which predicted that Gold = 25.3*Creep Score + 7499.6.\nimport pandas\nimport matplotlib\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nleague_data = pandas.read_csv('League_of_Legends_stats.csv')\nleague_data['Gold'] = league_data['Gold'].map(lambda s: int(s.replace(\",\", \"\")))\nleague_no_support = league_data[league_data['CS'] &gt;= 125]\nleague_no_support = league_no_support.sort_values(by='CS', ascending=True)\nplot = league_no_support.plot.scatter('CS', 'Gold', s=10, figsize=(15, 15))\nlin_reg = LinearRegression()\nlin_reg.fit(league_no_support['CS'].values.reshape(-1, 1), league_no_support['Gold'].values.reshape(-1, 1))\nline = lin_reg.predict(league_no_support['CS'].values.reshape(-1, 1))\nprint(\"Gold =\", lin_reg.coef_[0][0], \"cs +\", lin_reg.intercept_[0])\nplt.xlabel(\"Creep Score\")\nplt.title(\"Relationship between Creep Score and Gold\")\nleague_no_support['Gold Prediction'] = line\nplot.add_line(matplotlib.lines.Line2D(league_no_support['CS'], league_no_support['Gold Prediction']))\nplt.show()\n\nAfter plotting the linear regression, I wanted to know how well the model fit the data. So I made a new data column which took the absolute value of predicted gold subtracted from the creep score. Then dividing it by the creep score to get the percentage that it was off by. Taking the average of these values, I got that the linear regression is off by 443.9 gold or 3.8%. 3.8% is reasonably accurate.\nleague_no_support['diff'] = abs(league_no_support['Gold'] - league_no_support['Gold Prediction'])\nleague_no_support['diff percentage'] = league_no_support['diff'] / league_no_support['Gold'] * 100\nprint(\"Is on average off by\", np.average(league_no_support['diff']), \"gold\")\nprint(\"Is on average off by\", np.average(league_no_support['diff percentage']), \"%\")\nIn conclusion, due to the accuracy of the results, I would like to say that creep score can be related to gold earned in a game linearly. And that this prediction is accurately represents the data with it only being an average of 3.8% off.\nGithub View Source button takes you to the source code for the .qmd file.\nLinear Regression Jupiter Notebook"
  },
  {
    "objectID": "posts/Outlier_Detection/index.html",
    "href": "posts/Outlier_Detection/index.html",
    "title": "Outlier Detection Blog",
    "section": "",
    "text": "For my blog on Outlier Detection, similar to the project on linear regression, I used data League of Legends, but instead of using data from the competitive matchmaking, I used data from the professional tournament that happened in 2022. The outlier I detected was a high difference in average number of deaths over the tournament.\nI used python for this blog. This time, without taking out supports from the data and using average deaths as the delimiter, I made a bar chart which made an almost belle curve.\nimport pandas\nimport matplotlib\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nleague_data = pandas.read_csv('League_Worlds_2022.csv')\nsns.set_theme()\nsns.displot(data=league_data['Avg deaths']).set(title=\"Average Deaths\", xlabel=\"Average Deaths\",ylabel=\"Number of Players\")\nplt.show()\n\nAfter plotting it, I calculated the mean, and standard deviation. Then printed out the values that would be counted as outliers but checking if they are greater than three standard deviations away from the mean and printing those players names out. The two players that were statistical outliers in the abnormally high amount of deaths were Farfetch and Jelly.\nThe mean number of deaths were 2.8, with a standard deviation of 1.1. The upper limit being\n6 deaths and the lower limit being -0.5. Since the lower limit is in the negative, Farfetch and Jelly were over the upper limit at 6 deaths, with 6.4 and 6 average deaths respectively.\nmean = league_data['Avg deaths'].mean()\nstd = league_data['Avg deaths'].std()\nbot = mean - 3 * std\ntop = mean + 3 * std\nprint(\"Mean:\", mean)\nprint(\"Standard Dev:\", std)\nprint(\"Upper Limit:\", top)\nprint(\"Lower Limit:\", bot)\noutliers = league_data[(league_data['Avg deaths'] &lt; bot) | (league_data['Avg deaths'] &gt; top)]\nprint(outliers)\nGithub View Source button takes you to the source code for the .qmd file.\nOutlier Detection Jupiter Notebook"
  }
]